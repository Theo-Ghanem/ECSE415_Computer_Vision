{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "10da84490a6c5fa0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ECSE 415 - Assignment 3:  Classifiers, Object Recognition\n",
    "## Theo Ghanem 260972584\n",
    "## 1 CIFIAR10 Classification using SVM and Random Forest (50 points)\n",
    "### 1. Resize the train/test images to 64x64 and convert them to grayscale images. Compute HoG features with cells of 8x8 pixels, blocks of 4x4 cells, and 4 bins. This should generate a feature vector of size 1600 per image, which can be regarded as features for training classifiers.\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from more_itertools import locate\n",
    "# from google.colab import drive\n",
    "\n",
    "# SK-Learn\n",
    "from skimage.feature import hog\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# drive.mount('/content/drive')\n",
    "# \n",
    "filepath = '/Users/theog/Documents/CodingProjects/ECSE415_Computer_Vision/Assignments/A3/A3-W24-images/cifar-10-python/cifar-10-batches-py/'\n",
    "\n",
    "def unpickle(file):\n",
    "  with open(file, 'rb') as fo:\n",
    "    dict = pickle.load(fo, encoding='bytes')\n",
    "  return dict\n",
    "\n",
    "def resize_and_convert_to_grayscale(images, size=(64, 64)):\n",
    "  resized_images = []\n",
    "  for image in images:\n",
    "    # Resize image to 64x64\n",
    "    resized_image = cv2.resize(image, size)\n",
    "    # Convert image to grayscale\n",
    "    grayscale_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    resized_images.append(grayscale_image)\n",
    "  return np.array(resized_images)\n",
    "\n",
    "rawtestdata = unpickle(filepath + \"test_batch\")\n",
    "X_test = rawtestdata[b'data'].reshape(-1, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
    "\n",
    "rawdata = unpickle(filepath + \"data_batch_1\")\n",
    "X_trn = rawdata[b'data'].reshape(-1, 3, 32, 32).transpose(0,2,3,1).astype(\"uint8\")\n",
    "\n",
    "# Convert test images to grayscale and resize to 64x64\n",
    "trainImageData = resize_and_convert_to_grayscale(X_trn)\n",
    "testImageData = resize_and_convert_to_grayscale(X_test)\n",
    "\n",
    "#Compute HoG features with cells of 8x8 pixels, blocks of 4x4 cells, and 4 bins. This should generate a feature vector of size 1600 per image, which can be regarded as features for training classifiers.\n",
    "def compute_hog_features(images, cell_size=(8, 8), block_size=(4, 4), bins=4):\n",
    "  hog_features = []\n",
    "  for image in images:\n",
    "    # Compute HoG features\n",
    "    feature_vector = hog(image, pixels_per_cell=cell_size, cells_per_block=block_size, orientations=bins)\n",
    "    hog_features.append(feature_vector)\n",
    "  return np.array(hog_features)\n",
    "\n",
    "# Compute HoG features for the train and test images\n",
    "trainFeatures = compute_hog_features(trainImageData)\n",
    "testFeatures = compute_hog_features(testImageData)\n",
    "\n",
    "#print trainFeatures length:\n",
    "print(trainFeatures.shape)\n",
    "print(testFeatures.shape)\n",
    "### 2. Fit a non-linear SVM classifier with default hyperparameters on the features and the class features of the training images.\n",
    "clf = svm.SVC(kernel='rbf')\n",
    "clf.fit(trainFeatures, rawdata[b'labels'])\n",
    "### 3. Predict labels of the test images by feeding the test features to the trained classifier and calculate classification accuracy.\n",
    "\n",
    "predictedLabels = clf.predict(testFeatures)\n",
    "accuracy = accuracy_score(rawtestdata[b'labels'], predictedLabels)\n",
    "print(\"Classification accuracy: \", accuracy)\n",
    "### 4. Tune values of hyperparameters ’gamma’ and ’C’ to observe the accuracy change and select the hyperparameters with the highest test accuracy. Display your fine-tuning process by listing all the test cases with their parameter and corresponding accuracy.\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "grid = GridSearchCV(svm.SVC(), param_grid, refit=True, verbose=2)\n",
    "grid.fit(trainFeatures, rawdata[b'labels'])\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "### 5. Fit a Random Forest(RF) classifier (set n_estimators=10, max_depth=5, and criterion=’entropy’) on the features and the class labels of the training images.\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10, max_depth=5, criterion='entropy')\n",
    "rf.fit(trainFeatures, rawdata[b'labels'])\n",
    "### 6. Predict labels of the test images by feeding the test features to the trained classifier and calculate classification accuracy.\n",
    "\n",
    "predictedLabelsRF = rf.predict(testFeatures)\n",
    "accuracyRF = accuracy_score(rawtestdata[b'labels'], predictedLabelsRF)\n",
    "print(\"Classification accuracy: \", accuracyRF)\n",
    "### 7. Compare the performance of SVM and RF. Experiment training both classifiers with a range of random states(different values for random_state). Evaluate the stability within the random state. List the strengths and weaknesses of each model.\n",
    "\n",
    "svm_accuracies = []\n",
    "rf_accuracies = []\n",
    "for i in range(10):\n",
    "  # Train SVM classifier\n",
    "  clf = svm.SVC(kernel='rbf', random_state=i)\n",
    "  clf.fit(trainFeatures, rawdata[b'labels'])\n",
    "  predictedLabels = clf.predict(testFeatures)\n",
    "  accuracy = accuracy_score(rawtestdata[b'labels'], predictedLabels)\n",
    "  svm_accuracies.append(accuracy)\n",
    "\n",
    "  # Train RF classifier\n",
    "  rf = RandomForestClassifier(n_estimators=10, max_depth=5, criterion='entropy', random_state=i)\n",
    "  rf.fit(trainFeatures, rawdata[b'labels'])\n",
    "  predictedLabelsRF = rf.predict(testFeatures)\n",
    "  accuracyRF = accuracy_score(rawtestdata[b'labels'], predictedLabelsRF)\n",
    "  rf_accuracies.append(accuracyRF)\n",
    "  \n",
    "print(\"SVM accuracies: \", svm_accuracies)\n",
    "print(\"RF accuracies: \", rf_accuracies)\n",
    "\n",
    "# Strengths and weaknesses of each model\n",
    "# SVM:\n",
    "# Strengths:\n",
    "# - Effective in high dimensional spaces\n",
    "# - Memory efficient\n",
    "# - Versatile\n",
    "# Weaknesses:\n",
    "# - Not suitable for large datasets\n",
    "# - Not suitable for non-linear problems\n",
    "# - Sensitive to overfitting\n",
    "# RF:\n",
    "# Strengths:\n",
    "# - Effective in high dimensional spaces\n",
    "# - Memory efficient\n",
    "# - Versatile\n",
    "# - Not sensitive to overfitting\n",
    "# Weaknesses:\n",
    "# - Not suitable for large datasets\n",
    "# - Not suitable for non-linear problems"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d85d2d1d6a9db122"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
